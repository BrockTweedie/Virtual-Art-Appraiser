# Virtual Art Appraiser Data Wrangling

The analysis pipeline begins here. HTML from Sotheby's auction websites for 19th-Century European paintings are first cached locally item-by-item. (This is time-intensive if done from a single computer, since Sotheby's may block your IP address if their sites are visited faster than once per 15s.) Data contained in these HTML files are then scraped offline. The sites do not have a rigidly uniform format, so a bit of flexibility/micro-managing of the scraping procedure is required. The information is then compiled into Pandas DataFrames, and sent through a secondary cleaning step, e.g. to correct for different currencies and inflation. The full sample size is about 11,000 paintings from 3,000 artists. The most well-represented artist, Corot, has about 130 paintings.

For a small number of artists, O(10x) larger datasets were obtained using the auction aggregator site ArtNet. This is an expensive, industry-standard subscription service with a limited quota of searches, and implements measures to actively frustrate automated scraping. For these artists, a different scraping procedure was used, based on summary report websites generated through manual navigation of ArtNet's database. Data on about 3,000 paintings were extracted. Corot is again the most well-represented, with about 1,300 paintings.

There is also an image analysis pipeline under development, which is not included here. This faces a number of amusing issues, such the need to develop tools to remove frames and extraneous white borders.
